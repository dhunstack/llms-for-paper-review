Summary Of The Paper:
MoEs have been reported to be parameter inefficient such that larger models do not always lead to better performance.
This work proposes a parameter-efficient MoE models, by learning a soft combination of a global set of expert layers for each MoE layer.
Experimental results show that SaMoE improves parameter efficiency by reducing up to 5.2x parameters while obtaining strong pretraining and zeroshot generalization results.
Strength And Weaknesses:
Strengths:
The proposed method is simple in design and implementation but achieves reasonably good results.
Weaknesses:
The paper does not provide a fair comparison by fixing total parameters but ignore the computational cost (FLOPs) or activated parameters. In traditional MoE research, the general goal is to achieve better quality with a fixed computational cost (FLOPs), not with a fixed total parameters. The reviewer understand that this method provides a efficient way saving total parameters, but the reviewer suspects to achieve better quality, this method would also significantly increase the FLOPs compared to traditional top1 or top2 based routing used in switch transformer and GLaM.
Table 1 does not provide any details on computational cost.
According to multiple prior works including T5 and the Chinchilla [3], there is always a tradeoff between model capacity and training tokens, the larger the model is, the lower training data/steps can be achieved within the same computational cost budget.
According to Table 1, activated parameters are made fixed around 1B, however, it might not be clear about computational cost in this work's setting.
The paper misses fair comparisons with many significant related work including autoregressive sparse MoE, GLaM [1].
GLaM adopts a top-2 based routing, that can yield much better results than top-1 based routing.
Various efficient routing functions should be compared with in this work, as intelligent routing functions achieve similar effects of improving parameter efficiency.
For example, Expert Choice [2] routing achieves heterogeneous experts such that different tokens can utilize a variable number of parameters.
[1] https://arxiv.org/pdf/2112.06905.pdf
[2] https://arxiv.org/abs/2202.09368
[3] https://arxiv.org/abs/2203.15556
Clarity, Quality, Novelty And Reproducibility:
The paper is clearly written.
The proposed method is a minor improvement over traditional parameter sharing scheme like suggested in Universal Transformer.
The paper can be reproduced but the results are not valid.
Summary Of The Review:
No free lunch in deep learning: reducing parameters and reducing training time will not come for free without sacrificing model quality.
The paper's results are based on fixing total parameters but not on fixing computational cost (activated parameters and FLOPs), which can be unfair to many related works including GLaM.
The reviewer would not believe in any results that purely relying on parameter sharing, we could improve quality without introducing additional computational cost.
For example, whether this method increases activated parameters (experts per token) is unclear and should be explained.
For example, the paper can be increasing the number of layers or expert width or number of experts per token compared to GLaM.
All these increase activated parameters, thus inference time.
The paper should be also more proactive in explaining why quality gains can be achieved.