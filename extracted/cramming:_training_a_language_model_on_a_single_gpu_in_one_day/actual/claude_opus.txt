Summary of the Paper:
The paper investigates how much performance can be achieved by training a transformer-based language model from scratch on a single GPU in just one day, a challenge the authors call "Cramming".
They analyze nearly all components of the pretraining pipeline for this low-resource scenario and provide a modified pipeline with performance close to BERT.
They investigate why scaling down is hard and which modifications actually improve performance in this constrained setting.
The authors provide evidence that even with limited compute, performance closely follows scaling laws observed in large-compute settings.
Through the lens of scaling laws, they categorize recent improvements to training and architecture and discuss their merit and practical applicability for the limited compute setting.
Strengths and Weaknesses:
Strengths:
- The paper tackles an important and practical problem of training language models with very limited compute resources, which is relevant for most researchers and practitioners.
- The authors perform a comprehensive analysis of the pretraining pipeline, architectures, training setup, and datasets to find the best configuration for the low-resource setting.
- The empirical results showing that models trained in the limited compute regime can achieve performance close to BERT on GLUE tasks are convincing and significant.
- Analyzing the results through the lens of scaling laws provides useful insights into what matters most for performance in the constrained setting.
Weaknesses:
- While the paper is very thorough, some additional baselines could have strengthened the evaluation, such as comparing to other efficient models like TinyBERT, MobileBERT etc.
- The poor performance on the CoLA task is not sufficiently explained and requires more analysis to understand the limitations of the crammed models.
- Finetuning and evaluation is done only on the GLUE benchmark.
Expanding to a wider range of tasks would provide a more comprehensive assessment of the crammed models' capabilities.
- More discussion on how the findings can guide future research on efficient training of large language models would be useful.
Clarity, Quality, Novelty, and Reproducibility:
- The paper is clearly written and easy to follow.
The training setup, model architecture and evaluation are described in sufficient detail.
- The empirical methodology is sound and the results are convincing in demonstrating the feasibility of cramming language models.
- The idea of compressing the training of BERT-like models in a single-GPU single-day setup is novel and tackles a problem not sufficiently addressed in prior work.
- The authors plan to release code which would allow reproducing the results.
The experiments do not require any special hardware or data that is hard to obtain.
Summary of the Review:
This paper makes a valuable contribution by demonstrating that it is possible to train transformer language models that perform close to BERT while significantly compressing the time and compute requirements to a single GPU and a single day.
Through a comprehensive empirical analysis, the authors provide a training recipe that achieves this goal.
The paper also provides useful insights into what matters most for model quality in this constrained setting by examining the results through the lens of scaling laws.
The limitations are that evaluation is restricted to GLUE, and poor CoLA performance needs more analysis.
Overall, this is a significant result that would be of interest to the NLP community.
Some suggestions to further strengthen the paper would be to include additional efficient model baselines, expand evaluation beyond GLUE, and discuss more how the findings can guide future work on efficient training of large language models.