
{
    "1": {
        "summary": "The use of tensor decomposition for compressing transformers is considered an incremental improvement over previous research.",
        "verbatim": "This paper is an incremental improvement to previous state-of-the-art."
    },
    "2": {
        "summary": "Previous research has already discussed the decomposability and low-rank nature of FFN and MHA layers, as well as the use of tucker decomposition.",
        "verbatim": "Decomposability and low-rank nature of FFN and MHA layers has been discussed previously in the literature. The authors themselves refer to these prior works."
    },
    "3": {
        "summary": "The methodology's potential bias and the performance impact of using decomposed layers need more exploration.",
        "verbatim": "I recommend providing more insights into the workings of the method and the bias that the fixed structure like tucker decomposition can lead to. I also recommend exploring the systems impact of running training and inference using tucker decomposed layers."
    },
    "4": {
        "summary": "Concerns about the discrepancy in performance improvement despite significant parameter reduction.",
        "verbatim": "Explore and understand why finetuning with KD in Table 6 leads to such large accuracy drop."
    }
}
