
{
    "1": {
        "summary": "The experiments do not include the embedding layer and prediction layer, which makes the reported compression ratio misleading.",
        "verbatim": "The authors do not include embedding layer and prediction layer size in experiments, while only report the Transformer encoder size. I know that this can make the size of compressed model really amazing (e.g., 1.8M) and the compression ratio amazing (e.g., 86M/12.3M=7) but is not fair as the whole model including the embedding layer are used when deploying. If the embedding layer is added, the model size will increase a lot, and the compression ratio will decrease, which make the experimental results less surprising. But this should be made clear."
    },
    "2": {
        "summary": "The paper lacks comprehensive comparison with many related works.",
        "verbatim": "The authors name a lot of related works, but compare only very few of them in the experiments. Some other method(s) are missing in the related works. For example: [1]"
    }
}
