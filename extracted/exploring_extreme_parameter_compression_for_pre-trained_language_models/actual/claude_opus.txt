Summary of the Paper:
This paper explores extreme parameter compression for pre-trained language models (PLMs) like BERT using tensor decomposition methods.
The authors compare matrix decomposition, tensor train decomposition, and Tucker decomposition, concluding that Tucker decomposition is the most parameter-efficient for compression.
They propose decomposition and reconstruction protocols to improve the effectiveness and efficiency of Tucker decomposition.
Their compressed BERT models achieve comparable or slightly better performance than the original BERT on the GLUE benchmark, with significantly fewer parameters.
For example, one model performs on par with only 1/7 of the parameters in the Transformer layers, while a tiny version achieves 96.7% of BERT-base performance with 1/48 parameters and 2.7x faster inference.
The proposed method is shown to be orthogonal and complementary to other compression techniques like knowledge distillation.
Strengths and Weaknesses:
Strengths:
- Comprehensively compares different tensor decomposition methods for compressing PLMs, providing useful insights
- Achieves very high compression ratios (e.g.
1/48 parameters) while maintaining most of the performance, demonstrating the effectiveness of the Tucker decomposition approach
- Proposes practical decomposition and reconstruction protocols that enable efficient compression and inference
- Shows the method is complementary to knowledge distillation, indicating potential for combining with other compression techniques
Weaknesses:
- Evaluation is primarily on the GLUE benchmark; performance on other NLP tasks is not shown
- Theoretical analysis of why Tucker decomposition works well for compressing Transformers is somewhat limited
- Does not explore the limits of compression - even smaller models may be possible with some performance trade-off
- Impact on pre-training efficiency and resources is not analyzed, though this is very relevant for large-scale PLMs
Clarity, Quality, Novelty and Reproducibility:
The paper is clearly written and easy to follow.
The methodology is explained in detail and the experimental setup is reproducible.
The comprehensive comparison of different decomposition methods for PLMs is novel, and the extreme compression results achieved with Tucker decomposition are a significant contribution.
The overall quality of research and presentation is high.
Summary of the Review:
This paper makes a valuable contribution by demonstrating the effectiveness of Tucker tensor decomposition for extreme compression of PLMs like BERT.
The results showing comparable performance with 1/7 to 1/48 of the parameters are impressive and insightful.
The comparison between decomposition methods is rigorous and novel.
A few areas that could be further explored are evaluating on an even wider range of NLP tasks, providing more theoretical justification, analyzing impacts on pre-training, and pushing compression to the limit.
However, these do not take away from the strong technical contributions and clear presentation of the work.
Overall, this is a high-quality paper that advances the important problem of making large language models more efficient.