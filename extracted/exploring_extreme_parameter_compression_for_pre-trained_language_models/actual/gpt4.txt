Summary of the Paper
The paper proposes a novel approach to achieve larger compression ratios for pre-trained language models (PLMs), specifically focusing on BERT.
It identifies tensor decomposition, particularly Tucker decomposition, as a parameter-efficient method for compression.
The study introduces two decomposition and reconstruction protocols to enhance the effectiveness and efficiency of Tucker decomposition in parameter compression.
The compressed BERT models, with significantly reduced parameters, achieve comparable or slightly better performance than the original BERT on the GLUE benchmark.
Additionally, a tiny version of BERT, with less than 2M parameters (excluding the embedding layer) and 2.7 times faster inference, achieves 96.7% of BERT-base's performance.
The paper also examines the synergy of the proposed method with knowledge distillation, indicating its orthogonality to existing compression techniques.
Strengths and Weaknesses
Strengths:
The paper tackles the significant issue of PLM size, making a compelling case for the need for more efficient, smaller models without compromising performance.
It introduces an innovative application of Tucker decomposition for PLM compression, which is both novel and technically sound.
The empirical results are impressive, especially the achievement of a tiny BERT variant that retains most of the performance of the full model while being significantly smaller and faster.
Weaknesses:
The discussion on the scalability of the proposed approach as the sets produced by the decomposition grow larger is somewhat lacking.
There might be concerns about the reliability of probabilistic inference in such cases.
While the paper provides a good comparison with existing methods, it could benefit from a deeper theoretical discussion on why Tucker decomposition outperforms other techniques in this specific application.
Clarity, Quality, Novelty, and Reproducibility
The paper is well-written and clear, making it accessible to readers familiar with PLMs and tensor decomposition techniques.
The research is of high quality, employing rigorous experimental methodologies and providing a comprehensive comparison with baseline models.
The novelty lies in applying Tucker decomposition to compress PLMs efficiently, which is a relatively unexplored area.
The experiments appear reproducible, with sufficient details on the implementation and evaluation settings.
Summary of the Review
The paper presents a significant contribution to the field of NLP by proposing an effective method for compressing PLMs using Tucker decomposition.
Its approach not only preserves the performance of the original models on benchmark tasks but also improves inference speed, addressing a critical need for more efficient NLP models.
While the paper is strong in its empirical evaluation, further exploration into the theoretical underpinnings of its method and its scalability would enhance its contribution.