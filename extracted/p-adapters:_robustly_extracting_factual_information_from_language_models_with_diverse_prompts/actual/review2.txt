Summary Of The Paper:
The paper presents an approach for extracting factual information from LLMs, where authors discuss a user oriented setting where they take in varied natural language prompts and return the objects with information needs.
Authors proposed an approach, denoted P-Adapters, which is a model that is between the embedding layer and first attention layer of LLMs.
It takes LLM embeddings as input and output prompts used to query the LLM.
Authors also presented a Mixture of Experts models that learn a set of prompts and select one to query the LLM.
Experimenta results show that P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations.
The model and training procedure are well described and results are promising.
It is also great that the data used is in the Github repositories.
This said, the reader could benefit from better error analysis, as it is not clear how much hallucination and grounding the model is producing.
Main Review:
Authors also presented a Mixture of Experts models that learn a set of prompts and select one to query the LLM.
Experimenta results show that P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations.
The model and training procedure are well described and results are promising.
It is also great that the data used is in the Github repositories.
This said, the reader could benefit from better error analysis, as it is not clear how much hallucination and grounding the model is producing.
Summary Of The Review:
The obtained results and model itself will benefit the reader and researchers working in this important research area.