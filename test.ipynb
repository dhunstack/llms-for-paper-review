{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    all_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            # print(data)  # Process the JSON object as needed\n",
    "            all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "# Example usage\n",
    "filedata = read_jsonl('./data/ReviewCritique.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfiledata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "filedata['decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     accepted\n",
      "1     rejected\n",
      "2     rejected\n",
      "3     rejected\n",
      "4     rejected\n",
      "        ...   \n",
      "95    accepted\n",
      "96    accepted\n",
      "97    rejected\n",
      "98    rejected\n",
      "99    accepted\n",
      "Name: decision, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSONL file into a DataFrame\n",
    "df = pd.read_json('./data/ReviewCritique.jsonl', lines=True)\n",
    "\n",
    "# Access the 'decision' column\n",
    "decision_column = df['decision']\n",
    "\n",
    "# Display the DataFrame or the specific column\n",
    "print(decision_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['review', 'score'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review#1'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "entiretext = list(map(lambda x: x['segment_text'], df['review#1'][0]['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Of The Paper:\n",
      "The authors present a new \"meta\" prompting method to achieve better accuracy in few-shot prompting.\n",
      "This is based on the observation that certain types of questions tend to be more amenable to few-shot prompting (specifically, QA-type prompts). This prompting method (AMA), includes:\n",
      "Creating question() prompts, which transfer the original task into an open ended question\n",
      "Creating answer() prompts, which transfer the result to the actual answer They then aggregate a range of answers for a given prompt into a final answer, and find a significant performance lift\n",
      "Strength And Weaknesses:\n",
      "Strengths:\n",
      "This paper has impressive empirical results in that it up-levels the 6B parameter model to the accuracy of the 175B parameter model for most of the tasks\n",
      "In general, the observation that question-based tasks are much more likely to be answered correctly than other formats of tasks is really interesting.\n",
      "This is exactly the sort of quantitative/qualitative evaluation of benchmark datasets that the field needs more of: we often take benchmark datasets for granted, and even though we know that they're flawed in various ways, we still use them to define SOTA.\n",
      "Similarly, thinking about what sorts of text features and distributions work \"well\" with LLMs (e.g., formatting a task as a natural language question rather than templating it in less natural ways) is often overlooked, even though it has significant impact on the accuracy.\n",
      "The error analysis section in the appendix is good-- much more useful for future work than just presenting a raw accuracy number.\n",
      "Weaknesses:\n",
      "My main concern is that I'm not sure this is enough for a full paper. This is a useful analysis and prompt engineering strategy, but I would expect either a deeper analysis of why formulating things as Q/A works so much better (e.g., analysis of the training data), or\n",
      "I'm confused about the question() prompt. I thought these contained \"task-agnostic examples of how to transform statements to various questions\", but it seems like the format is not the same for all tasks? (Appendix H)\n",
      "Clarity, Quality, Novelty And Reproducibility:\n",
      "This paper is clearly written and seems to be reproducible given that the authors release the prompts and code.\n",
      "See Strengths and Weaknesses section for comments on novelty.\n",
      "Summary Of The Review:\n",
      "At a high level, this paper is starting to push back against the idea that LLMs are agnostic to prompt structure (ie, that prompt structure is a unimportant implementation detail), which is an important step for the field.\n",
      "That being said, it feels a bit more like an analysis of single prompt engineering strategy than a full paper, and for that reason, I am on the fence for acceptance.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(entiretext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Of The Paper:\n",
      "This paper focuses on zero-shot setting and looks into generating prompts that will improve zero shot performance.\n",
      "It has been known that LLMs behave very differently given even slightly different prompts, and a lot of effort can be spent finding the perfect prompt.\n",
      "Authors propose to, given the input, generate several \"imperfect\" prompts for this input and then aggregate the results.\n",
      "To generate the imperfect prompts, authors come up with a number of templates (question template and answer templates) that teach model to, given an input, generate the questions for the input and then to answer these questions. The final prediction is an aggregation of prediction from the answer step.\n",
      "The aggregation happens via weak supervision (the model for this is also learnt).\n",
      "The experiments show impressive improvements for smaller models, allowing them to match that of larger models few shot performance.\n",
      "Additionally, authors provide some insights as to what makes the good prompt.\n",
      "For example, they find that open ended prompts do better than prompts with suggested restricted output\n",
      "Strength And Weaknesses:\n",
      "Pros:\n",
      "impressive results (zero shot performance of smaller models is comparable or better than few shot of larger) on context dependent tasks\n",
      "the idea is really neat cons:\n",
      "not flashed out enough to allow me to reproduce/code it up (without looking at authors' code)\n",
      "clarity (please see more detailed comments)\n",
      "less impressive results on closed book tasks\n",
      "not sure how much effort/tuning goes into prompt genereation and WS\n",
      "Clarity, Quality, Novelty And Reproducibility:\n",
      "Clarity:\n",
      "ok I really found this paper hard to follow.\n",
      "After reading the main paper the reader is left wondering as to what was actually proposed.\n",
      "The intro should really focus more on Figure 1 and highlight the overall algorithm better (generate the questions, generate the answers for these generated questions, aggregate the output).\n",
      "I feel that Figure 1+ Appendix F should form a good chunk of the main paper, showing what happens, why you believe that questions and answers that were generated by AMA are of good quality.\n",
      "Additionally, apart from \"insights\" as to what makes good questions, authors don't provide enough details of what is being generated.\n",
      "How many templates do you end up using? Is it task dependent? Were these templates selected for each dataset manually? Finally, the weak supervision should be also described in the main paper, as it is an integral part of the approach.\n",
      "Novelty:\n",
      "after having (finally) understood the approach, I do find it witty.\n",
      "Authors are generating a number of prompts that hopefully highlight different angles of the input, and combine the outputs of the model based on these prompts to come up with a final solution.\n",
      "Reproducibility:\n",
      "see my comments on clarity. As far as I understand authors open source their code (I have not seen where???) so this should help reproducibility, but overall just by reading the paper, I don't think I would have enough details to reproduce the approach.\n",
      "Summary Of The Review:\n",
      "Update: in light of additional comparison with prompt tuning, i am raising my score.\n",
      "I appreciate all the additional experiments authors ran.\n",
      "I still encourage to improve the flow/the story/the presentation, but the method deserves to be seem\n",
      "Update: i am keeping my score of marginally above. I do think the narrative can be improved, but it is also an interesting peace of work and it is pretty witty to make the model generate the questions and answers to improve the prompting.\n",
      "Overall It is a very good idea of forcing the model to write the prompts that it is able to respond to.\n",
      "I do think the main paper should be reworked to include more examples/explanations of what is being proposed plus WS part.\n",
      "Additional questions\n",
      "Are all the models you tested are next token (perfix) trained, not span corruption trained correct?\n",
      "With respect to having less of improvement on closed book tasks - do you think it is because that the answers you show in the \"anwers\" part of the chain don't draw on closed book knowledge?\n",
      "I assume your \"answers\" templates are the same for various tasks so they don't include that closed book knowledge?\n",
      "What is the complexity (time and space) of weak supervision?\n",
      "You say in Table 1 that prompts can abstain from the predictions. What does it mean? Don't you get an output for each of your prompt chains?\n",
      "Are questions prompts and answer prompts task specific? How did you create them How many chains do you create for each input during inference?\n",
      "Building on this Isn't it the case that AMA performance will depend greatly on question() step? If the questions are not well done then when you create answer prompts, the model will not produce the result you are looking for\n",
      "This seems to work well on simpler tasks (yes or no, question answering), but i am struggling to understand how the combining of the results will work for more generative tasks like summary tasks where variance of the predictions will be much higher\n"
     ]
    }
   ],
   "source": [
    "entiretext = list(map(lambda x: x['segment_text'], df['review#2'][0]['review']))\n",
    "print('\\n'.join(entiretext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmbeam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
